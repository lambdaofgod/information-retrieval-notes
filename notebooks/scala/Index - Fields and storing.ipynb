{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.lucene:lucene-core:7.2.1`\n",
    "import $ivy.`org.apache.lucene:lucene-queries:7.2.1`\n",
    "import $ivy.`org.apache.lucene:lucene-queryparser:7.2.1`\n",
    "import $ivy.`org.apache.lucene:lucene-analyzers-common:7.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.lucene.analysis.standard.StandardAnalyzer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.lucene.document._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.lucene.index._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.lucene.store.RAMDirectory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.lucene.util.BytesRef\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.lucene.analysis.standard.StandardAnalyzer\n",
    "import org.apache.lucene.document._\n",
    "import org.apache.lucene.index._\n",
    "import org.apache.lucene.store.RAMDirectory\n",
    "import org.apache.lucene.util.BytesRef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this to convert some iterator-like Java objects to Scala collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoScalaStream\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toScalaStream[T](iter: {def next(): T}): Stream[T] = {\n",
    "  val value = iter.next()\n",
    "  if (value == null) Stream.empty[T]\n",
    "  else value #:: toScalaStream(iter)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Information retrieval\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "*Index* is a central concept in IR. Sometimes called an inverted file, index is a datastructure optimized for search.\n",
    "\n",
    "Let's think about files at a high level: they can be understood as mappings from integers to words (when we define separators).\n",
    "\n",
    "Namely, files with contents \"Alice has a cat\", \"cat chases mice.\"\n",
    "\n",
    "Form mappings\n",
    "\n",
    "| Index | File1 | File2 |\n",
    "| ----- |:----:| :---: |\n",
    "|  0 | Alice| cat |\n",
    "|  1 | has| chases|\n",
    "|  2 | a| a|\n",
    "|  3 | cat| mouse |\n",
    "\n",
    "At this level index is inversion of this (hence the name) - given a word, you can find numbers of files that contain it.\n",
    "\n",
    "This becomes\n",
    "\n",
    "| Word | Files |\n",
    "| ----  |:---: |\n",
    "| Alice | 1 |\n",
    "| has | 1|\n",
    "| a | 1,2|\n",
    "| cat| 1, 2 |\n",
    "| chases| 2 |\n",
    "| mouse | 2|\n",
    "\n",
    "This is important for Information Retrieval, because if we optimize our datastructure for fetching file indices given words, we'll also have a way of retrieving files for more complex queries (for example \"Alice OR cat\") by using operations on retrieved collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# Lucene\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index setup\n",
    "\n",
    "To write to index, we'll need to define analyzer and IndexWriter.\n",
    "\n",
    "We'll explore analyzer part in next notebook. For now it suffices to tell that analyzer is responsible for preprocessing strings by converting to some canonical form (like for example splitting them into words and lowercasing) - it's also called *normalization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36manalyzer\u001b[39m: \u001b[32mStandardAnalyzer\u001b[39m = org.apache.lucene.analysis.standard.StandardAnalyzer@537e4b20\n",
       "\u001b[36mindexWriter\u001b[39m: \u001b[32mIndexWriter\u001b[39m = org.apache.lucene.index.IndexWriter@7fa75d73"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexWriter = new IndexWriter(\n",
    "  new RAMDirectory(),\n",
    "  new IndexWriterConfig(analyzer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents and Field Types \n",
    "\n",
    "When we defined indices, we used files for an example.\n",
    "\n",
    "But what about metadata, like title or author name?\n",
    "\n",
    "Lucene doesn't actually store files - it stores `Documents`. Documents are key-value mappings - for example contents are stored under key (field) `content`.\n",
    "\n",
    "`FieldTypes` are used to define exactly what is stored for a field. For example in the following code we define `FieldType` which we're going to use for our `content` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtextFieldType\u001b[39m: \u001b[32mFieldType\u001b[39m = stored,indexed,tokenized,termVector"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFieldType = new FieldType()\n",
    "\n",
    "textFieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)\n",
    "textFieldType.setTokenized(true)\n",
    "textFieldType.setStored(true)\n",
    "textFieldType.setStoreTermVectors(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m7L\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val contents = Array(\n",
    "  \"Humpty Dumpty sat on a wall,\",\n",
    "  \"Humpty Dumpty had a great fall.\",\n",
    "  \"All the king's horses and all the king's men\",\n",
    "  \"Couldn't put Humpty together again.\"\n",
    ")\n",
    "\n",
    "contents.foreach { content =>\n",
    "  val doc = new Document()\n",
    "  \n",
    "  doc.add(\n",
    "    new Field(\n",
    "      \"content\",\n",
    "      content,\n",
    "      textFieldType)\n",
    "  )\n",
    "  indexWriter.addDocument(doc)\n",
    "}\n",
    "\n",
    "indexWriter.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we indexed something, let's see what gets actually stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mindexReader\u001b[39m: \u001b[32mDirectoryReader\u001b[39m = StandardDirectoryReader(segments_1:4 _0(7.2.1):c4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexReader = DirectoryReader.open(\n",
    "  indexWriter.getDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      "dumpty humpty sat wall \n",
      "Document 1\n",
      "dumpty fall great had humpty \n",
      "Document 2\n",
      "all horses king's men \n",
      "Document 3\n",
      "again couldn't humpty put together \n"
     ]
    }
   ],
   "source": [
    "(0 until indexReader.maxDoc) foreach { i =>\n",
    "  \n",
    "  val terms = indexReader.getTermVector(i, \"content\")\n",
    "  val termsIterator = terms.iterator()\n",
    "  val termsStream = toScalaStream(terms.iterator()).map(_.utf8ToString())\n",
    "  println(s\"Document $i\")\n",
    "  termsStream.foreach { term =>\n",
    "    print(term + \" \")\n",
    "  }\n",
    "  println()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that not everything is stored. Why? It's not just any words that weren't stored - we'll look into this in the following notebook."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
