{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import *\n",
    "from org.apache.lucene.index import *\n",
    "from org.apache.lucene.store import RAMDirectory\n",
    "from org.apache.lucene.util import BytesRefIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f8ef938b168>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Information retrieval\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "*Index* is a central concept in IR. Sometimes called an inverted file, index is a datastructure optimized for search.\n",
    "\n",
    "Let's think about files at a high level: they can be understood as mappings from integers to words (when we define separators).\n",
    "\n",
    "Namely, files with contents \"Alice has a cat\", \"cat chases mice.\"\n",
    "\n",
    "Form mappings\n",
    "\n",
    "| Index | File1 | File2 |\n",
    "| ----- |:----:| :---: |\n",
    "|  0 | Alice| cat |\n",
    "|  1 | has| chases|\n",
    "|  2 | a| a|\n",
    "|  3 | cat| mouse |\n",
    "\n",
    "At this level index is inversion of this (hence the name) - given a word, you can find numbers of files that contain it.\n",
    "\n",
    "This becomes\n",
    "\n",
    "| Word | Files |\n",
    "| ----  |:---: |\n",
    "| Alice | 1 |\n",
    "| has | 1|\n",
    "| a | 1,2|\n",
    "| cat| 1, 2 |\n",
    "| chases| 2 |\n",
    "| mouse | 2|\n",
    "\n",
    "This is important for Information Retrieval, because if we optimize our datastructure for fetching file indices given words, we'll also have a way of retrieving files for more complex queries (for example \"Alice OR cat\") by using operations on retrieved collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# Lucene\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index setup\n",
    "\n",
    "To write to index, we'll need to define analyzer and IndexWriter.\n",
    "\n",
    "We'll explore analyzer part in next notebook. For now it suffices to tell that analyzer is responsible for preprocessing strings by converting to some canonical form (like for example splitting them into words and lowercasing) - it's also called *normalization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer()\n",
    "index_writer = IndexWriter(\n",
    "    RAMDirectory(),\n",
    "    IndexWriterConfig(analyzer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents and Field Types \n",
    "\n",
    "When we defined indices, we used files for an example.\n",
    "\n",
    "But what about metadata, like title or author name?\n",
    "\n",
    "Lucene doesn't actually store files - it stores `Documents`. Documents are key-value mappings - for example contents are stored under key (field) `content`.\n",
    "\n",
    "`FieldTypes` are used to define exactly what is stored for a field. For example in the following code we define `FieldType` which we're going to use for our `content` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field_type = FieldType()\n",
    "\n",
    "text_field_type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)\n",
    "text_field_type.setTokenized(True)\n",
    "text_field_type.setStored(True)\n",
    "text_field_type.setStoreTermVectors(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [\n",
    "  \"Humpty Dumpty sat on a wall,\",\n",
    "  \"Humpty Dumpty had a great fall.\",\n",
    "  \"All the king's horses and all the king's men\",\n",
    "  \"Couldn't put Humpty together again.\"\n",
    "]\n",
    "\n",
    "for content in contents:\n",
    "    doc = Document()\n",
    "    \n",
    "    doc.add(Field('content', content, text_field_type))\n",
    "    \n",
    "    index_writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we indexed something, let's see what gets actually stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexReader = DirectoryReader.open(\n",
    "  index_writer.getDirectory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dumpty', 'humpty', 'sat', 'wall']\n",
      "\n",
      "['dumpty', 'fall', 'great', 'had', 'humpty']\n",
      "\n",
      "['all', 'horses', \"king's\", 'men']\n",
      "\n",
      "['again', \"couldn't\", 'humpty', 'put', 'together']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(indexReader.maxDoc()):\n",
    "    terms = indexReader.getTermVector(i, \"content\")\n",
    "    \n",
    "    term_list = \n",
    "    print(list(term.utf8ToString() for term in BytesRefIterator.cast_(terms.iterator())))\n",
    "#        print(term.utf8ToString())\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that not everything is stored. Why? It's not just any words that weren't stored - we'll look into this in the following notebook."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
